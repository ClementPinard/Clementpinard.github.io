{"componentChunkName":"component---src-templates-post-template-js","path":"/depthnet/","result":{"data":{"mdx":{"id":"909e63aa-e927-542b-98ba-a91bac7089ed","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Introducing DepthNet\",\n  \"description\": \"We present DepthNet, the convolutional neural netowrk introduced in our paper named \\\"End-to-end depth from motion with stabilized monocular videos\\\"\",\n  \"date\": \"2017-09-10\",\n  \"template\": \"post\",\n  \"authors\": [{\n    \"name\": \"Cl√©ment Pinard\",\n    \"website\": \"/\",\n    \"lab\": {\n      \"name\": \"ENSTA Paris - U2IS\",\n      \"url\": \"http://u2is.ensta-paris.fr/index.php?lang=en\"\n    }\n  }, {\n    \"name\": \"Antoine Manzanera\",\n    \"website\": \"https://perso.ensta-paris.fr/~manzaner/\",\n    \"lab\": {\n      \"name\": \"ENSTA Paris - U2IS\",\n      \"url\": \"http://u2is.ensta-paris.fr/index.php?lang=en\"\n    }\n  }, {\n    \"name\": \"David Filliat\",\n    \"website\": \"https://perso.ensta-paris.fr/~filliat/eng/index.html\",\n    \"lab\": {\n      \"name\": \"ENSTA Paris - U2IS\",\n      \"url\": \"http://u2is.ensta-paris.fr/index.php?lang=en\"\n    }\n  }, {\n    \"name\": \"Laure Chevalley\",\n    \"lab\": {\n      \"name\": \"Parrot\",\n      \"url\": \"http://parrot.com\"\n    }\n  }]\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar Paper = makeShortcode(\"Paper\");\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"figure\", null, mdx(\"video\", {\n    autoPlay: true,\n    loop: true\n  }, mdx(\"source\", {\n    src: \"/f28a3b890cfe139dc66cf30805f17b5a/depthnet.webm\",\n    type: \"video/webm\"\n  })), \"DepthNet results vizualisation. Top left : Input video with added reticule, bottom left : Ground truth, bottom right : DepthNet output, top right: Error map (green means 0)\"), mdx(\"h1\", {\n    \"id\": \"abstract\"\n  }, \"Abstract\"), mdx(\"p\", null, \"We propose a depth map inference system from monocular videos based on a novel dataset for navigation that mimics aerial footage from gimbal stabilized monocular camera in rigid scenes. Unlike most navigation datasets, the lack of rotation implies an easier structure from motion problem which can be leveraged for different kinds of tasks such as depth inference and obstacle avoidance. We also propose an architecture for end-to-end depth inference with a fully convolutional network. Results show that although tied to camera inner parameters, the problem is locally solvable and leads to good quality depth prediction.\"), mdx(\"p\", null, \"we also propose a multi-range architecture for unconstrained UAV flight, leveraging flight data from sensors to make accurate depth maps for uncluttered outdoor environment. We try our algorithm on both synthetic scenes and real UAV flight data. Quantitative results are given for synthetic scenes with a slightly noisy orientation, and show that our multi-range architecture improves depth inference.\"), mdx(\"h1\", {\n    \"id\": \"papers\"\n  }, \"Papers\"), mdx(\"p\", null, \"Two papers were published for this project\"), mdx(Paper, {\n    mdxType: \"Paper\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"826px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"141.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAcABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAEDAgX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAH3pxTRBm0BR//EABgQAAMBAQAAAAAAAAAAAAAAAAABMRAR/9oACAEBAAEFAtdEOinDiz//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAUEAEAAAAAAAAAAAAAAAAAAAAw/9oACAEBAAY/Ak//xAAbEAACAgMBAAAAAAAAAAAAAAAAASExEBFRYf/aAAgBAQABPyFsnuH8EcRQszabo8BQf//aAAwDAQACAAMAAAAQc8oM/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPxAf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPxAf/8QAHRABAAICAgMAAAAAAAAAAAAAAQARITFRcUGBkf/aAAgBAQABPxBLQXHUG2cOyGt3AuVtxBqOX5NevUN4mKiBZjiaLiqUKxbwgBQUT//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"picture\", {\n    parentName: \"span\"\n  }, \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/dc8f24cdaef00a60cb54742d082c9152/8ac56/ArticleUAVg.pdf.webp 240w\", \"/static/dc8f24cdaef00a60cb54742d082c9152/d3be9/ArticleUAVg.pdf.webp 480w\", \"/static/dc8f24cdaef00a60cb54742d082c9152/40616/ArticleUAVg.pdf.webp 826w\"],\n    \"sizes\": \"(max-width: 826px) 100vw, 826px\",\n    \"type\": \"image/webp\"\n  }), \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/dc8f24cdaef00a60cb54742d082c9152/09b79/ArticleUAVg.pdf.jpg 240w\", \"/static/dc8f24cdaef00a60cb54742d082c9152/7cc5e/ArticleUAVg.pdf.jpg 480w\", \"/static/dc8f24cdaef00a60cb54742d082c9152/1c58d/ArticleUAVg.pdf.jpg 826w\"],\n    \"sizes\": \"(max-width: 826px) 100vw, 826px\",\n    \"type\": \"image/jpeg\"\n  }), \"\\n          \", mdx(\"img\", {\n    parentName: \"picture\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"src\": \"/static/dc8f24cdaef00a60cb54742d082c9152/1c58d/ArticleUAVg.pdf.jpg\",\n    \"alt\": \"UAVg\",\n    \"title\": \"UAVg\",\n    \"loading\": \"lazy\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    }\n  }), \"\\n        \"), \"\\n    \")), mdx(\"div\", null, mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"End-to-end depth from motion with stabilized monocular videos\")), mdx(\"p\", null, \"in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://uavg17.ipb.uni-bonn.de/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"UAVg 2017\"), \" (Oral)\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"pdf/UAVg2017/end_to_end_depth_from_motion_with_stabilized_monocular_videos.pdf\"\n  }, \"Full Text\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"pdf/UAVg2017/Oral-UAVg17-Cpinard.pdf\"\n  }, \"Slides\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.04453\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"ArXiV\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://hal.archives-ouvertes.fr/hal-01587652\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Hal\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#citation\"\n  }, \"cite\")))), mdx(\"hr\", null), mdx(Paper, {\n    mdxType: \"Paper\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"960px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"141.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAcABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAIBAwT/xAAVAQEBAAAAAAAAAAAAAAAAAAABAP/aAAwDAQACEAMQAAAB9PfraWxUasZAf//EABoQAQEAAwEBAAAAAAAAAAAAAAECABEhEgP/2gAIAQEAAQUCv4KkoS8Td65gJlV5mH0b7oSeH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8BH//EABcRAAMBAAAAAAAAAAAAAAAAAAABEGH/2gAIAQIBAT8Bq0//xAAdEAACAQQDAAAAAAAAAAAAAAAAATECEBEhInGh/9oACAEBAAY/AtV+jTfG2cvoWySRvEEW3b//xAAcEAEAAgIDAQAAAAAAAAAAAAABACERMUFRkcH/2gAIAQEAAT8hSU5fEOLlwP2AbesbCMDQqU6BzAhBs2szjkdLloYXOLBuLQKYtAE//9oADAMBAAIAAwAAABAwCzP/xAAXEQEBAQEAAAAAAAAAAAAAAAAAARFh/9oACAEDAQE/ELGp1j//xAAYEQADAQEAAAAAAAAAAAAAAAAAARFBUf/aAAgBAgEBPxCkG3gznR//xAAdEAEBAAICAwEAAAAAAAAAAAABEQAhMVFBYXGR/9oACAEBAAE/EHJWqjQ9PuAHD2tvZuRsy62uDE4JKe3FCtQKSvtzRw/DLAFkZ5X1nBmKbH5guUESmDeI0sy4zkMJAy+8/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"picture\", {\n    parentName: \"span\"\n  }, \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/6afb7166706683e6c59e820fba1f8521/8ac56/ArticleECMR.pdf.webp 240w\", \"/static/6afb7166706683e6c59e820fba1f8521/d3be9/ArticleECMR.pdf.webp 480w\", \"/static/6afb7166706683e6c59e820fba1f8521/e46b2/ArticleECMR.pdf.webp 960w\", \"/static/6afb7166706683e6c59e820fba1f8521/f992d/ArticleECMR.pdf.webp 1440w\", \"/static/6afb7166706683e6c59e820fba1f8521/882b9/ArticleECMR.pdf.webp 1920w\", \"/static/6afb7166706683e6c59e820fba1f8521/b3915/ArticleECMR.pdf.webp 3305w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"type\": \"image/webp\"\n  }), \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/6afb7166706683e6c59e820fba1f8521/09b79/ArticleECMR.pdf.jpg 240w\", \"/static/6afb7166706683e6c59e820fba1f8521/7cc5e/ArticleECMR.pdf.jpg 480w\", \"/static/6afb7166706683e6c59e820fba1f8521/6a068/ArticleECMR.pdf.jpg 960w\", \"/static/6afb7166706683e6c59e820fba1f8521/644c5/ArticleECMR.pdf.jpg 1440w\", \"/static/6afb7166706683e6c59e820fba1f8521/0f98f/ArticleECMR.pdf.jpg 1920w\", \"/static/6afb7166706683e6c59e820fba1f8521/34c43/ArticleECMR.pdf.jpg 3305w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"type\": \"image/jpeg\"\n  }), \"\\n          \", mdx(\"img\", {\n    parentName: \"picture\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"src\": \"/static/6afb7166706683e6c59e820fba1f8521/6a068/ArticleECMR.pdf.jpg\",\n    \"alt\": \"UAVg\",\n    \"title\": \"UAVg\",\n    \"loading\": \"lazy\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    }\n  }), \"\\n        \"), \"\\n    \")), mdx(\"div\", null, mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Multi range Real-time depth inference from a monocular stabilized footage using a Fully Convolutional Neural Network\")), mdx(\"p\", null, \"in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://ecmr2017.ensta-paristech.fr\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"ECMR 2017\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"pdf/ECMR2017/Multi_range_Real_time_depth_inference_from_a_monocular_stabilized_footage_using_a_Fully_Convolutional_Neural_Network.pdf\"\n  }, \"Full Text\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"pdf/ECMR2017/poster_ECMR_cpinard.pdf\"\n  }, \"Poster\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.04467\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"ArXiV\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://hal.archives-ouvertes.fr/hal-01587652\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Hal\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#citation\"\n  }, \"Cite\")))), mdx(\"h1\", {\n    \"id\": \"additional-results\"\n  }, \"Additional Results\"), mdx(\"figure\", null, mdx(\"div\", {\n    \"className\": \"gatsby-resp-iframe-wrapper\",\n    \"style\": {\n      \"paddingBottom\": \"62.5%\",\n      \"position\": \"relative\",\n      \"height\": \"0\",\n      \"overflow\": \"hidden\",\n      \"marginBottom\": \"1.0725rem\"\n    }\n  }, \" \", mdx(\"iframe\", {\n    parentName: \"div\",\n    \"src\": \"https://www.youtube.com/embed/nU-Gv_I7zhg\",\n    \"frameBorder\": \"0\",\n    \"allowFullScreen\": true,\n    \"style\": {\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\",\n      \"width\": \"100%\",\n      \"height\": \"100%\"\n    }\n  }), \" \")), mdx(\"h1\", {\n    \"id\": \"code\"\n  }, \"Code\"), mdx(\"p\", null, \"Training code is available on Github\"), mdx(\"undefined\", null, mdx(\"a\", {\n    \"href\": \"https://github.com/ClementPinard/DepthNet\",\n    \"style\": {\n      \"marginRight\": \"auto\"\n    }\n  }, mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"https://shields.io/badge/ClementPinard/DepthNet-gray?logo=github&style=flat-square\",\n    \"alt\": \"Github Repo\",\n    \"height\": 70\n  })), mdx(\"br\", null), mdx(\"a\", {\n    \"href\": \"https://github.com/ClementPinard/DepthNet/stargazers\"\n  }, mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"https://img.shields.io/github/stars/ClementPinard/DepthNet.svg?style=social&label=Star\",\n    \"alt\": \"Github Stars\",\n    \"height\": 50\n  })), mdx(\"a\", {\n    \"href\": \"https://github.com/ClementPinard/DepthNet/network\"\n  }, mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"https://img.shields.io/github/forks/ClementPinard/DepthNet.svg?style=social&label=Forks\",\n    \"alt\": \"Github Forks\",\n    \"height\": 50\n  }))), mdx(\"h1\", {\n    \"id\": \"still-box-dataset\"\n  }, \"Still Box Dataset\"), mdx(\"video\", {\n    autoPlay: true,\n    loop: true\n  }, mdx(\"source\", {\n    src: \"/ccce2bfb328624092624dee6eb62f99d/dataset.webm\",\n    type: \"video/webm\"\n  })), mdx(\"p\", null, \"The \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Still Box Dataset\"), \" has been used to train our network. It is available to download \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://stillbox.ensta.fr/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"here\"), \". It consists in 4 different image sizes. Here is a brief recap of sizes\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Image Size\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"number of scenes\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"total size (GB)\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"compressed size (GB)\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"64x64\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"80K\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"19\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"9.8\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"128x128\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"16K\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"12\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"7.1\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"256x256\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"3.2K\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"8.5\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"5\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"512x512\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"3.2K\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"33\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"19\")))), mdx(\"p\", null, \"Get more information on the official website : \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://stillbox.ensta.fr/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"https://stillbox.ensta.fr/\")), mdx(\"h1\", {\n    \"id\": \"citation\"\n  }, \"Citation\"), mdx(\"p\", null, \"If you use DepthNet in your research, please add the following references.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"text\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-text\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-text\"\n  }, \"@Article{depthnet_uavg,\\n    AUTHOR = {Pinard, Cl{\\\\'e}ment and Chevalley, Laure and\\n              Manzanera, Antoine and Filliat, David},\\n    TITLE = {end-to-end depth from motion with\\n             stabilized monocular videos},\\n    JOURNAL = {ISPRS Annals of Photogrammetry,\\n               Remote Sensing and Spatial Information Sciences},\\n    VOLUME = {IV-2/W3},\\n    YEAR = {2017},\\n    PAGES = {67--74},\\n    DOI = {10.5194/isprs-annals-IV-2-W3-67-2017}\\n  }\"))), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"text\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-text\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-text\"\n  }, \"@inproceedings{depthnet_ecmr,\\n    TITLE = {{Multi range Real-time depth inference from a monocular stabilized\\n              footage using a Fully Convolutional Neural Network}},\\n    AUTHOR = {Pinard, Cl{\\\\'e}ment and Chevalley, Laure\\n              and Manzanera, Antoine and Filliat, David},\\n    URL = {https://hal.archives-ouvertes.fr/hal-01587658},\\n    BOOKTITLE = {{European Conference on Mobile Robotics}},\\n    ADDRESS = {Paris, France},\\n    ORGANIZATION = {{ENSTA ParisTech}},\\n    YEAR = {2017},\\n    MONTH = Sep,\\n    KEYWORDS = {Deep CNN ;  HDR ;  Drone ;  Depth},\\n    PDF = {https://hal.archives-ouvertes.fr/hal-01587658/file/Article%20ECMR.pdf},\\n    HAL_ID = {hal-01587658},\\n    HAL_VERSION = {v1}\\n  }\"))));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/depthnet/"},"frontmatter":{"date":"2017-09-10","description":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"We present DepthNet, the convolutional neural netowrk introduced in our paper named \\u201CEnd-to-end depth from motion with stabilized monocular videos\\u201D\"));\n}\n;\nMDXContent.isMDXComponent = true;","title":"Introducing DepthNet","socialImage":null,"authors":[{"name":"Cl√©ment Pinard","website":"/","lab":{"name":"ENSTA Paris - U2IS","url":"http://u2is.ensta-paris.fr/index.php?lang=en"}},{"name":"Antoine Manzanera","website":"https://perso.ensta-paris.fr/~manzaner/","lab":{"name":"ENSTA Paris - U2IS","url":"http://u2is.ensta-paris.fr/index.php?lang=en"}},{"name":"David Filliat","website":"https://perso.ensta-paris.fr/~filliat/eng/index.html","lab":{"name":"ENSTA Paris - U2IS","url":"http://u2is.ensta-paris.fr/index.php?lang=en"}},{"name":"Laure Chevalley","website":null,"lab":{"name":"Parrot","url":"http://parrot.com"}}]}}},"pageContext":{"slug":"/depthnet/","page":2}},"staticQueryHashes":["317903835"]}