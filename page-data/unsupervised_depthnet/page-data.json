{"componentChunkName":"component---src-templates-post-template-js","path":"/unsupervised_depthnet/","result":{"data":{"mdx":{"id":"2f67249f-912e-5981-b062-5145b59d8131","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Learning Structure from Motion from motion\",\n  \"description\": \"Following Depthnet, we present a way to train it with self-supervision, with an algorithm introduced in the paper \\\"Learning Structure-from-motion *from motion*\\\"\",\n  \"date\": \"2018-09-10\",\n  \"template\": \"post\",\n  \"authors\": [{\n    \"name\": \"Clément Pinard\",\n    \"website\": \"/\",\n    \"lab\": {\n      \"name\": \"ENSTA Paris - U2IS\",\n      \"url\": \"http://u2is.ensta-paris.fr/index.php?lang=en\"\n    }\n  }, {\n    \"name\": \"Antoine Manzanera\",\n    \"website\": \"https://perso.ensta-paris.fr/~manzaner/\",\n    \"lab\": {\n      \"name\": \"ENSTA Paris - U2IS\",\n      \"url\": \"http://u2is.ensta-paris.fr/index.php?lang=en\"\n    }\n  }, {\n    \"name\": \"David Filliat\",\n    \"website\": \"https://perso.ensta-paris.fr/~filliat/eng/index.html\",\n    \"lab\": {\n      \"name\": \"ENSTA Paris - U2IS\",\n      \"url\": \"http://u2is.ensta-paris.fr/index.php?lang=en\"\n    }\n  }, {\n    \"name\": \"Laure Chevalley\",\n    \"lab\": {\n      \"name\": \"Parrot\",\n      \"url\": \"https://www.parrot.com/\"\n    }\n  }]\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar Paper = makeShortcode(\"Paper\");\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"figure\", null, mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"960px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"42.5%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"picture\", {\n    parentName: \"span\"\n  }, \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/cca28b95aca34a62b348d9a41fb0d701/8ac56/figure.webp 240w\", \"/static/cca28b95aca34a62b348d9a41fb0d701/d3be9/figure.webp 480w\", \"/static/cca28b95aca34a62b348d9a41fb0d701/e46b2/figure.webp 960w\", \"/static/cca28b95aca34a62b348d9a41fb0d701/f992d/figure.webp 1440w\", \"/static/cca28b95aca34a62b348d9a41fb0d701/36ae5/figure.webp 1496w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"type\": \"image/webp\"\n  }), \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/cca28b95aca34a62b348d9a41fb0d701/8ff5a/figure.png 240w\", \"/static/cca28b95aca34a62b348d9a41fb0d701/e85cb/figure.png 480w\", \"/static/cca28b95aca34a62b348d9a41fb0d701/d9199/figure.png 960w\", \"/static/cca28b95aca34a62b348d9a41fb0d701/07a9c/figure.png 1440w\", \"/static/cca28b95aca34a62b348d9a41fb0d701/e4ee8/figure.png 1496w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"type\": \"image/png\"\n  }), \"\\n          \", mdx(\"img\", {\n    parentName: \"picture\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"src\": \"/static/cca28b95aca34a62b348d9a41fb0d701/d9199/figure.png\",\n    \"alt\": \"figure\",\n    \"title\": \"figure\",\n    \"loading\": \"lazy\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    }\n  }), \"\\n        \"), \"\\n    \"), \"\\nUnsupervised DepthNet training workflow\\n\"), mdx(\"h1\", {\n    \"id\": \"abstract\"\n  }, \"Abstract\"), mdx(\"p\", null, \"This work is based on a questioning of the quality metrics used by deep neural networks performing depth prediction from a single image, and then of the usability of recently published works on unsupervised learning of depth from videos. These works are all predicting depth from a single image, thus it is only known up to an undetermined scale factor, which is not sufficient for practical use cases that need an absolute depth map, i.e. the determination of the scaling factor.\"), mdx(\"p\", null, \"To overcome these limitations, we propose to learn in the same unsupervised manner a depth map inference system from monocular videos that takes a pair of images as input. This algorithm actually learns structure-from-motion from motion, and not only structure from context appearance. The scale factor issue is explicitly treated, and the absolute depth map can be estimated from camera displacement magnitude, which can be easily measured from cheap external sensors.\"), mdx(\"p\", null, \"Our solution is also much more robust with respect to domain variation and adaptation via fine tuning, because it does not rely entirely on depth from context. Two use cases are considered, unstabilized moving camera videos, and stabilized ones. This choice is motivated by the UAV (for Unmanned Aerial Vehicle) use case that generally provides reliable orientation measurement.\"), mdx(\"p\", null, \"We provide a set of experiments showing that, used in real conditions where only speed can be known, our network outperforms competitors for most depth quality measures. Results are given on the well known KITTI dataset, which provides robust stabilization for our second use case, but also contains moving scenes which are very typical of the in-car road context.\"), mdx(\"p\", null, \"We then present results on a synthetic dataset that we believe to be more representative of typical UAV scenes. Lastly, we present two domain adaptation use cases showing superior robustness of our method compared to single view depth algorithms, which indicates that it is better suited for highly variable visual contexts.\"), mdx(\"h1\", {\n    \"id\": \"paper\"\n  }, \"Paper\"), mdx(Paper, {\n    mdxType: \"Paper\"\n  }, mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"960px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"141.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAcABQDASIAAhEBAxEB/8QAGQAAAwEBAQAAAAAAAAAAAAAAAAIDBAEF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAQD/2gAMAwEAAhADEAAAAfXecY1GUarvwUKkf//EABoQAQACAwEAAAAAAAAAAAAAAAEAAhASIRH/2gAIAQEAAQUCHByeksrau6OopwIkK8n/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAbEAACAgMBAAAAAAAAAAAAAAABIQARECAxQf/aAAgBAQAGPwLmTyIqlU9jAe3/xAAdEAEAAgICAwAAAAAAAAAAAAABABEhMUFRYYGR/9oACAEBAAE/Ibfd3K5qBFZ9sc7g5zqJFt/AsWQmBADydzpV4lhBe/sApn7An//aAAwDAQACAAMAAAAQPw2P/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQMBAT8QIf/EABcRAAMBAAAAAAAAAAAAAAAAAAABEBH/2gAIAQIBAT8QiZp//8QAHRABAAICAwEBAAAAAAAAAAAAAQARITFBUWGBof/aAAgBAQABPxCw1hkVVZjoDT7iEwr1FjWytSpldS1WhKPga5jRaGtHR5HiSyAb9cyh0AAA4mYs+sGb2bsQxKWr21/YQan/2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"picture\", {\n    parentName: \"span\"\n  }, \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/f225c99891d0602ff9064bc75ae2e30c/8ac56/thumb.webp 240w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/d3be9/thumb.webp 480w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/e46b2/thumb.webp 960w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/f992d/thumb.webp 1440w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/882b9/thumb.webp 1920w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/b3915/thumb.webp 3305w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"type\": \"image/webp\"\n  }), \"\\n          \", mdx(\"source\", {\n    parentName: \"picture\",\n    \"srcSet\": [\"/static/f225c99891d0602ff9064bc75ae2e30c/09b79/thumb.jpg 240w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/7cc5e/thumb.jpg 480w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/6a068/thumb.jpg 960w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/644c5/thumb.jpg 1440w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/0f98f/thumb.jpg 1920w\", \"/static/f225c99891d0602ff9064bc75ae2e30c/34c43/thumb.jpg 3305w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"type\": \"image/jpeg\"\n  }), \"\\n          \", mdx(\"img\", {\n    parentName: \"picture\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"src\": \"/static/f225c99891d0602ff9064bc75ae2e30c/6a068/thumb.jpg\",\n    \"alt\": \"UAVg\",\n    \"title\": \"UAVg\",\n    \"loading\": \"lazy\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    }\n  }), \"\\n        \"), \"\\n    \")), mdx(\"div\", null, mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Learning Structure From Motion From Motion\")), mdx(\"p\", null, \"In \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://sites.google.com/site/deepgeometry2018/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"GMDL Workshop\"), \" at \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://eccv2018.org/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"ECCV 2018\")), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"pdf/ECCV2018/Learning_structure_from_motion_from_motion.pdf\"\n  }, \"Full Text\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"pdf/ECCV2018/Poster_ECCV.pdf\"\n  }, \"Poster\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1809.04471\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"ArXiV\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://hal.archives-ouvertes.fr/hal-01587652\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow noopener noreferrer\"\n  }, \"Hal\"), \" / \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"#citation\"\n  }, \"Cite\")))), mdx(\"h1\", {\n    \"id\": \"additional-results\"\n  }, \"Additional results\"), mdx(\"figure\", null, mdx(\"div\", {\n    \"className\": \"gatsby-resp-iframe-wrapper\",\n    \"style\": {\n      \"paddingBottom\": \"62.5%\",\n      \"position\": \"relative\",\n      \"height\": \"0\",\n      \"overflow\": \"hidden\",\n      \"marginBottom\": \"1.0725rem\"\n    }\n  }, \" \", mdx(\"iframe\", {\n    parentName: \"div\",\n    \"src\": \"https://www.youtube.com/embed/ZDgWAWTwU7U\",\n    \"frameBorder\": \"0\",\n    \"allowFullScreen\": true,\n    \"style\": {\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\",\n      \"width\": \"100%\",\n      \"height\": \"100%\"\n    }\n  }), \" \")), mdx(\"h1\", {\n    \"id\": \"code\"\n  }, \"Code\"), mdx(\"p\", null, \"Training code is available on Github\"), mdx(\"undefined\", null, mdx(\"a\", {\n    \"href\": \"https://github.com/ClementPinard/Unsupervised-DepthNet\",\n    \"style\": {\n      \"marginRight\": \"auto\"\n    }\n  }, mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"https://shields.io/badge/ClementPinard/Unsupervised--DepthNet-gray?logo=github&style=flat-square\",\n    \"alt\": \"Github Repo\",\n    \"height\": 70\n  })), mdx(\"br\", null), mdx(\"a\", {\n    \"href\": \"https://github.com/ClementPinard/Unsupervised-DepthNet/stargazers\"\n  }, mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"https://img.shields.io/github/stars/ClementPinard/Unsupervised-DepthNet.svg?style=social&label=Star\",\n    \"alt\": \"Github Stars\",\n    \"height\": 50\n  })), mdx(\"a\", {\n    \"href\": \"https://github.com/ClementPinard/Unsupervised-DepthNet/network\"\n  }, mdx(\"img\", {\n    parentName: \"a\",\n    \"src\": \"https://img.shields.io/github/forks/ClementPinard/Unsupervised-DepthNet.svg?style=social&label=Forks\",\n    \"alt\": \"Github Forks\",\n    \"height\": 50\n  }))), mdx(\"h1\", {\n    \"id\": \"citation\"\n  }, \"Citation\"), mdx(\"p\", null, \"If you use unsupervised DepthNet in your research, please add the following reference.\"), mdx(\"div\", {\n    \"className\": \"gatsby-highlight\",\n    \"data-language\": \"text\"\n  }, mdx(\"pre\", {\n    parentName: \"div\",\n    \"className\": \"language-text\"\n  }, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-text\"\n  }, \"@inproceedings{pinard2018learning,\\n  title={Learning structure-from-motion from motion},\\n  author={Pinard, Cl{\\\\'e}ment and Chevalley, Laure and Manzanera, Antoine and Filliat, David},\\n  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},\\n  year={2018}\\n}\"))));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/unsupervised_depthnet/"},"frontmatter":{"date":"2018-09-10","description":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Following Depthnet, we present a way to train it with self-supervision, with an algorithm introduced in the paper \\u201CLearning Structure-from-motion \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"from motion\"), \"\\u201D\"));\n}\n;\nMDXContent.isMDXComponent = true;","title":"Learning Structure from Motion from motion","socialImage":null,"authors":[{"name":"Clément Pinard","website":"/","lab":{"name":"ENSTA Paris - U2IS","url":"http://u2is.ensta-paris.fr/index.php?lang=en"}},{"name":"Antoine Manzanera","website":"https://perso.ensta-paris.fr/~manzaner/","lab":{"name":"ENSTA Paris - U2IS","url":"http://u2is.ensta-paris.fr/index.php?lang=en"}},{"name":"David Filliat","website":"https://perso.ensta-paris.fr/~filliat/eng/index.html","lab":{"name":"ENSTA Paris - U2IS","url":"http://u2is.ensta-paris.fr/index.php?lang=en"}},{"name":"Laure Chevalley","website":null,"lab":{"name":"Parrot","url":"https://www.parrot.com/"}}]}}},"pageContext":{"slug":"/unsupervised_depthnet/","page":2}},"staticQueryHashes":["317903835"]}